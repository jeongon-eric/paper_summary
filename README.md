# Beyond Correlation: Interpretable Evaluation of MT Metrics (arXiv:2410.05183)

**ì €ì:** Stefano Perrella*, Lorenzo Proietti*, Pere-LluÃ­s Huguet Cabot, Edoardo Barba, Roberto Navigli (Sapienza NLP Group)  
**ë°œí–‰:** 2024.10.07 | [[PDF]](https://arxiv.org/pdf/2410.05183) | *ìš”ì•½ by Eric ğŸ™ | 2026.02.09*

ì´ READMEëŠ” ë…¼ë¬¸ ì„¹ì…˜ë³„ë¡œ **ì˜ì–´ ì›ë¬¸ ë°œì·Œ** â†’ **í•œêµ­ì–´ ìš”ì•½** â†’ **ì›ë¬¸ ì¬ë°œì·Œ (í‚¤ í¬ì¸íŠ¸)** â†’ **ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸** í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í–ˆì–´. ê·¸ë¦¼ë„ paper_imagesì—ì„œ ê´€ë ¨ëœ ê±¸ ì‚¬ì´ì‚¬ì´ì— ë¼ì›Œë„£ìŒ. ì „ì²´ í…ìŠ¤íŠ¸ëŠ” \`paper_images/paper1.txt\`ì—ì„œ ì¶”ì¶œ (ì „ì²´ 1560ì¤„, ì—¬ê¸°ì„  ì£¼ìš” ì„¹ì…˜ë§Œ). PPT ëŒ€ì‹  ì´ê²Œ ë” ê¹”ë”í•  ê±° ê°™ì•„!

![paper1_all-01.png](paper_images/paper1_all-01.png)  
*(í‘œì§€ & Fig1: COMET ìŠ¤ì½”ì–´ 0.86ì´ \"ì¢‹ì€ì§€\" í•´ì„ ì–´ë ¤ìš´ ì˜ˆì‹œ)*

## Abstract (ì´ˆë¡)

### ì˜ì–´ ì›ë¬¸ (Full)
\`\`\`
Machine Translation (MT) evaluation metrics assess translation quality automatically. Recently, researchers have employed MT metrics for various new use cases, such as data filtering and translation re-ranking. However, most MT metrics return assessments as scalar scores that are difficult to interpret, posing a challenge to making informed design choices. Moreover, MT metricsâ€™ capabilities have historically been evaluated using correlation with human judgment, which, despite its efficacy, falls short of providing intuitive insights into metric performance, especially in terms of new metric use cases. To address these issues, we introduce an interpretable evaluation framework for MT metrics. Within this framework, we evaluate metrics in two scenarios that serve as proxies for the data filtering and translation re-ranking use cases. Furthermore, by measuring the performance of MT metrics using Precision, Recall, and F-score, we offer clearer insights into their capabilities than correlation with human judgments. Finally, we raise concerns regarding the reliability of manually curated data following the Direct Assessments+Scalar Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with Multidimensional Quality Metrics (MQM) annotations.
\`\`\`

### í•œêµ­ì–´ ìš”ì•½
MT ë©”íŠ¸ë¦­(ì˜ˆ: COMET)ì´ ë‹¨ìˆœ ìŠ¤ì¹¼ë¼ ì ìˆ˜ë§Œ ë±‰ì–´ì„œ í•´ì„ ì–´ë µê³ , ê¸°ì¡´ ìƒê´€ê´€ê³„(correlation) í‰ê°€ë¡œëŠ” ë°ì´í„° í•„í„°ë§/ë¦¬ë­í‚¹ ê°™ì€ ìƒˆ ìš©ë„ì— ì•½í•¨. **P/R/F1 ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ ì œì•ˆ**ìœ¼ë¡œ ë” ì§ê´€ì  í‰ê°€. DA+SQM ì¸ê°„ ë°ì´í„°ë„ MQMê³¼ ë‚®ì€ ì¼ì¹˜ë„.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
most MT metrics return assessments as scalar scores that are difficult to interpret [...] we introduce an interpretable evaluation framework [...] using Precision, Recall, and F-score
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
ìŠ¤ì¹¼ë¼ ì ìˆ˜(ì˜ˆ: COMET=0.86 â†’ \"ì¢‹ìŒ?\") ë¬¸ì œ í•´ê²°. **F1ë¡œ í•„í„°ë§/ë¦¬ë­í‚¹ í”„ë¡ì‹œ í‰ê°€**. ì½”ë“œ ê³µê°œ: [GitHub](https://github.com/SapienzaNLP/interpretable-mt-metrics-eval). ë¦¬ë·°ì–´ë“¤í•œí…Œ \"correlation ë„ˆë¨¸\"ë¡œ ì–´í•„ ê°•í•¨!

## 1. Introduction (ì„œë¡ )

### ì˜ì–´ ì›ë¬¸ ë°œì·Œ
\`\`\`
Over the past few years, Machine Translation (MT) evaluation metrics have transitioned from heuristic-based to neural-based [...] new MT metrics use cases have emerged. [...] the lack of a dedicated evaluation, paired with the inherent opacity of MT metrics, makes it challenging to determine whether one metric suits a given task better [...]
\`\`\`

### í•œêµ­ì–´ ìš”ì•½
MT ë©”íŠ¸ë¦­ì´ neuralë¡œ ì—…ê·¸ë ˆì´ë“œëì§€ë§Œ, **data filtering, re-ranking, RL reward** ìƒˆ ìš©ë„ì—ì„œ ìŠ¤ì¹¼ë¼ ì ìˆ˜ ë¶ˆíˆ¬ëª…. ìƒê´€ê´€ê³„ í‰ê°€ë§Œìœ¼ë¡œëŠ” ë¶€ì¡± â†’ **í•´ì„ ê°€ëŠ¥í•œ í‰ê°€ í•„ìš”**.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
data filtering [...] translation re-ranking [...] lack of a dedicated evaluation [...] we address these issues by introducing a novel and more interpretable evaluation framework
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
ê¸°ì¡´ WMT Metrics Shared Task ì–¸ê¸‰í•˜ë©° gap ì§€ì . Fig1 ì˜ˆì‹œì²˜ëŸ¼ \"0.86ì´ 0.43ì˜ 2ë°° ì¢‹ì€ê°€?\" ë¬¸ì œ ì œê¸°. **ì‹¤ë¬´ì ìœ„í•œ threshold ì¶”ì²œ** í¬ì¸íŠ¸!

![cropped_fig1.png](paper_images/cropped_fig1.png)  
*(Fig1 í´ë¡œì¦ˆì—…: scalar vs interpretable)*

## 2. The Interpretability of MT Metricsâ€™ Assessments (ë©”íŠ¸ë¦­ í‰ê°€ì˜ í•´ì„ì„±)

### ì˜ì–´ ì›ë¬¸ ë°œì·Œ
\`\`\`
In the field of AI, Interpretability is defined as â€œthe ability to explain [...]â€ [...] we are concerned with the interpretability of their assessments. Specifically, most state-of-the-art MT metrics [...] return assessments as scalar quality scores [...] we attribute MT metrics assessmentsâ€™ lack of interpretability to three main factors: 1. Range consistency [...] 2. Error attribution [...] 3. Performance [...]
\`\`\`

### í•œêµ­ì–´ ìš”ì•½
**3ëŒ€ ë¬¸ì œ**: 1) ë²”ìœ„ ì¼ê´€ì„± ë¶€ì¡± (ì ìˆ˜ ì°¨ì´ ì˜ë¯¸ ëª¨ë¦„), 2) ì—ëŸ¬ ê·€ì† ë¶ˆê°€, 3) ìƒê´€ê´€ê³„ë§Œìœ¼ë¡œ ì„±ëŠ¥ ë¶ˆëª…í™•. MaTESe/xCOMET/GEMBA-MQMì²˜ëŸ¼ í•´ì„ ì‹œë„í–ˆì§€ë§Œ trade-off ìˆìŒ.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
Range consistency: unclear whether a difference in metric score has the same meaning [...] Error attribution: scalar quality assessments do not identify specific translation errors. Performance: correlation with human judgment [...]
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
**í•´ê²°ì±…**: í•„í„°ë§/ë¦¬ë­í‚¹ í”„ë¡ì‹œë¡œ P/R/F1 ì¸¡ì •. GEMBA-MQMì€ GPT-4 ë¹„ì‹¸ê³  ì¬í˜„ì„± ë‚®ìŒ ì§€ì .

## 3. An Interpretable Evaluation Framework (í‰ê°€ í”„ë ˆì„ì›Œí¬)

### ì˜ì–´ ì›ë¬¸ ë°œì·Œ
\`\`\`
Two popular new MT metrics applications are data filtering and translation re-ranking. [...] we evaluate MT metrics performance in two settings: i) when metrics are used as binary classifiers [...] ii) when metrics are used to identify the best translation [...]
\`\`\`
*(3.1 Binary Classifiers: GOOD/BAD ë¶„ë¥˜, Ï„ thresholdë¡œ P/R/F1. 3.2 Re-Ranking: RRP Precision)*

### í•œêµ­ì–´ ìš”ì•½
**ë°ì´í„° í•„í„°ë§ í”„ë¡ì‹œ**: ë©”íŠ¸ë¦­ì„ binary classifierë¡œ (Ï„ ì´ìƒ GOOD). **P_MÏ„ = Pr(GOOD | score â‰¥ Ï„)** ë“±. ë¦¬ë­í‚¹: ìµœê³  ë²ˆì—­ topì— ë‘ëŠ”ì§€ RRP. FÎ² (Î²=âˆš(1/2))ë¡œ Precision ì¤‘ì‹œ.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
Metrics as Binary Classifiers for Data Filtering: M(t) â‰¥ Ï„ â†’ GOOD [...] F_MÏ„ = (3 P R) / (2âˆš(1/2) P + R) [...] Re-Ranking Precision: |T^M âˆ© T^H| / |T^M|
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
ìˆ˜ì‹ ì§ê´€ì ! False positiveì´ ë°ì´í„° í’ˆì§ˆ ë§ì¹˜ëŠ” ì´ìœ  ì„¤ëª…. **ì‹¤ì „ threshold íŠœë‹ ê°€ì´ë“œ**.

![cropped_paper1_graph.png](paper_images/cropped_paper1_graph.png)  
*(ê·¸ë˜í”„: P/R trade-off ì˜ˆì‹œ)*

## 4. Experimental Setup (ì‹¤í—˜ ì„¤ì •)

### ì˜ì–´ ì›ë¬¸ ë°œì·Œ
\`\`\`
We employ WMT23MQM [...] WMT23DA+SQM [...] Metrics: COMET, BLEURT-20, MetricX-23-*, xCOMET-*, MaTESe-*, GEMBA-MQM [...] Thresholds: max F on test/dev [...]
\`\`\`

### í•œêµ­ì–´ ìš”ì•½
**ë°ì´í„°**: WMT23MQM (MQM ì–´ë…¸í…Œì´ì…˜), dev=WMT22MQM. **GOOD: MQM â‰¥ -4 (Major error 0ê°œ)**, PERFECT â‰¥ -1. 15ê°œ MT ì‹œìŠ¤í…œ ë²ˆì—­ ë¹„êµ.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
GOOD if h â‰¥ -4 (no Major errors, â‰¤4 Minor) [...] Metrics: 14ê°œ (ref-based/free + random baseline)
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
ì¸ê°„ ì„±ëŠ¥(DA+SQM vs MQM)ë„ baselineìœ¼ë¡œ. MetricX ê³µê°œ ëª¨ë¸ ì¶”ì²œ.

## 5. Results (ê²°ê³¼)

### ì˜ì–´ ì›ë¬¸ ë°œì·Œ (Table1 ìš”ì•½)
\`\`\`
Table 1: [...] GEMBA-MQM F=81.59 (GOOD/BAD), xCOMET-QE-ENSEMBLE 81.40 [...] Re-ranking: xCOMET-ENSEMBLE RRP=43.17%
\`\`\`

### í•œêµ­ì–´ ìš”ì•½
**GOOD/BAD F1 ìµœê³ **: GEMBA-MQM 81.6%, xCOMET 81.4%. **Ref-free > ref-based? No, ref-based ìš°ìœ„**. Ref-free ì¶”ì²œ: MetricX-23-QE-XL. Re-ranking RRP 30-43%, ì¸ê°„ -0.67 vs ë©”íŠ¸ë¦­ -2.5.

### ì˜ì–´ ì›ë¬¸ ì¬ë°œì·Œ (Key Quote)
\`\`\`
Ref-free â‰¥ ref-based? [...] best openly available ref-free: MetricX-23-QE-XL [...] Thresholds unstable across langs/datasets
\`\`\`

### ì¶”ê°€ ìš”ì•½/ì¸ì‚¬ì´íŠ¸
**False positive ë¶„ì„ (Fig2)**: ìµœê³  ë©”íŠ¸ë¦­ë„ Minor error 3ê°œ í‰ê· . DA+SQM ì¸ê°„ë„ F1 75%ë¡œ ë©”íŠ¸ë¦­ë³´ë‹¤ ì•½í•¨! Threshold dev setìœ¼ë¡œ íŠœë‹ ê¶Œì¥.

![cropped_paper1_fig3.png](paper_images/cropped_paper1_fig3.png)  
*(Fig3: Threshold ë³€ë™ì„±)*

![cropped_table1.png](paper_images/cropped_table1.png)  
*(Table1: ì£¼ìš” ê²°ê³¼ í…Œì´ë¸”)*

## Discussion & Takeaways (ë…¼ì˜/ì‹œì‚¬ì )
- **ì¶”ì²œ**: Data filtering â†’ MetricX-23-QE-XL (ref-free). Re-ranking â†’ ref-based + MBR.
- **ë¬¸ì œ**: Threshold ì–¸ì–´/ë°ì´í„°ì…‹ ë”°ë¼ ë¶ˆì•ˆì • â†’ ëŒ€ëŸ‰ ì–´ë…¸í…Œì´ì…˜ìœ¼ë¡œ ì¶”ì •.
- **ë¯¸ë˜**: ë” ë§ì€ ìš©ë„ í”„ë¡ì‹œ, stable threshold ì—°êµ¬.

**ì „ì²´ ê¸¸ì´**: ~10í˜ì´ì§€. seminarìš©ìœ¼ë¡œ ë”±! ë” ë””í…Œì¼ í•„ìš”í•˜ë©´ \`paper_images/paper1.txt\` í’€ í…ìŠ¤íŠ¸ ë´. ì´ê±° GitHub READMEë¡œ ì˜¬ë¦¬ë©´ ì™„ë²½í•  ë“¯? ğŸ˜